{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running inference tools\n",
    "\n",
    "As machine learning (ML) becomes more popular in HEP analysis, `coffea` also\n",
    "provide tools to assist with using ML tools within the coffea framework. For\n",
    "training and validation, you would likely need custom data mangling tools to\n",
    "convert HEP data formats ([NanoAOD][nanoaod], [PFNano][pfnano]) to a format that\n",
    "best interfaces with the ML tool of choice, as for training and validation, you\n",
    "typical want to have fine control over what computation is done. For more\n",
    "advanced use cases of data mangling and data saving, refer to the [awkward array\n",
    "manual][datamangle] and [uproot][uproot_write]/[parquet][ak_parquet] write\n",
    "operations for saving intermediate states. The helper tools provided in coffea\n",
    "focuses on ML inference, where ML tool outputs are used as another variable to\n",
    "be used in the event/object selection chain.\n",
    "\n",
    "[nanoaod]: https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookNanoAOD\n",
    "[pfnano]: https://github.com/cms-jet/PFNano\n",
    "[datamangle]: https://awkward-array.org/doc/main/user-guide/how-to-restructure.html\n",
    "[uproot_write]: https://uproot.readthedocs.io/en/latest/basic.html#writing-ttrees-to-a-file\n",
    "[ak_parquet]: https://awkward-array.org/doc/main/reference/generated/ak.to_parquet.html\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why these wrapper tools are needed\n",
    "\n",
    "The typical operation of using ML inference tools in the awkward/coffea analysis\n",
    "tools involves the conversion and padding of awkward array to ML tool containers\n",
    "(usually something that is `numpy`-compatible), run the inference, then\n",
    "convert-and-truncate back into the awkward array syntax required for the\n",
    "analysis chain to continue. With awkward arrays' laziness now being handled\n",
    "entirely by [`dask`][dask_awkward], the conversion operation of awkward array to\n",
    "other array types needs to be wrapped in a way that is understandable to `dask`.\n",
    "The packages in the `ml_tools` package attempts to wrap the common tools used by\n",
    "the HEP community with a common interface to reduce the verbosity of the code on\n",
    "the analysis side.\n",
    "\n",
    "[dask_awkward]: https://dask-awkward.readthedocs.io/en/stable/gs-limitations.html\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using ParticleNet-like jet variable calculation using PyTorch\n",
    "\n",
    "The example given in this notebook be using [`pytorch`][pytorch] to calculate a\n",
    "jet-level discriminant using its constituent particles. An example for how to\n",
    "construct such a `pytorch` network can be found in the docs file, but for\n",
    "`mltools` in coffea, we only support the [TorchScript][pytorch] format files to\n",
    "load models to ensure operability when scaling to clusters. Let us first start\n",
    "by downloading the example ParticleNet model file and a small `PFNano`\n",
    "compatible file, and a simple function to open the `PFNano` with and without\n",
    "dask.\n",
    "\n",
    "\n",
    "[pytorch]: https://pytorch.org/\n",
    "[pytorch_jit]: https://pytorch.org/tutorials/beginner/saving_loading_models.html#export-load-model-in-torchscript-format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --quiet -O model.pt https://github.com/CoffeaTeam/coffea/raw/ml_tools/tests/samples/triton_models_test/pn_test/1/model.pt\n",
    "\n",
    "!wget --quiet -O pfnano.root https://github.com/yimuchen/coffea/raw/ml_tools/tests/samples/pfnano.root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.nanoevents import NanoEventsFactory\n",
    "from coffea.nanoevents.schemas import PFNanoAODSchema\n",
    "\n",
    "\n",
    "def open_events(permit_dask=False):\n",
    "    return NanoEventsFactory.from_root(\n",
    "        \"file:./pfnano.root\",\n",
    "        schemaclass=PFNanoAODSchema,\n",
    "        permit_dask=permit_dask,\n",
    "    ).events()\n",
    "\n",
    "events = open_events(permit_dask=True)\n",
    "print(events.Jet.constituents.pf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare a class to handle inference request by extending the\n",
    "`mltools.torch_wrapper` class. As the base class cannot know anything about the\n",
    "data mangling required for the users particular model, we will need to overload\n",
    "at least the method `prepare_awkward_to_numpy`:\n",
    "\n",
    "- The input can be an arbitrary number of awkward arrays. In this example, we\n",
    "  will be passing in the event array.\n",
    "- The output should be single tuple `a` + single dictionary `b`, this is to\n",
    "  ensure that arbitrarily complicated outputs can be passed to the underlying\n",
    "  `pytorch` model instance like `model(*a, **b)`. The contents of `a` and `b`\n",
    "  should be `numpy`-compatible _awkward_ arrays (so awkward arrays that can be\n",
    "  trivially converted to `numpy` arrays via a `ak.to_numpy` call). In this\n",
    "  ParticleNet-like example, the model expects the following inputs:\n",
    "\n",
    "  - A `N` jets x `2` coordinate x `100` constituents \"points\" array,\n",
    "    representing the constituent coordinates.\n",
    "  - A `N` jets x `5` feature x `100` constituents \"features\" array, representing\n",
    "    the constituent features of interest to be used for inference.\n",
    "  - A `N` jets x `1` mask x `100` constituent \"mask\" array, representing whether\n",
    "    a constituent should be masked from the inference request.\n",
    "\n",
    "  In this case, we will need to flatten the `E` events x `N` jets structure, as\n",
    "  well as stack the constituent attributes of interest, into a single array.\n",
    "\n",
    "After defining this minimum class, we can attempt to run an inference using the\n",
    "`__call__` method defined in the base class. Notice that overloading this single\n",
    "method will automatically allow for the inference to be called on both awkward\n",
    "and dask-awkward. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ensc/VirtualENV/coffea-test/lib/python3.8/site-packages/coffea/ml_tools/helper.py:155: UserWarning: No format checks were performed on input!\n",
      "  warnings.warn(\"No format checks were performed on input!\")\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\n\nSee if this has been reported at https://github.com/scikit-hep/awkward/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/awkward/operations/ak_transform.py:431\u001b[0m, in \u001b[0;36mtransform\u001b[0;34m(transformation, array, depth_context, lateral_context, allow_records, broadcast_parameters_rule, left_broadcast, right_broadcast, numpy_to_regular, regular_to_jagged, return_value, highlevel, behavior, *more_arrays)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[39mwith\u001b[39;00m ak\u001b[39m.\u001b[39m_errors\u001b[39m.\u001b[39mOperationErrorContext(\n\u001b[1;32m    413\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mak.transform\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    414\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m     },\n\u001b[1;32m    430\u001b[0m ):\n\u001b[0;32m--> 431\u001b[0m     \u001b[39mreturn\u001b[39;00m _impl(\n\u001b[1;32m    432\u001b[0m         transformation,\n\u001b[1;32m    433\u001b[0m         array,\n\u001b[1;32m    434\u001b[0m         more_arrays,\n\u001b[1;32m    435\u001b[0m         depth_context,\n\u001b[1;32m    436\u001b[0m         lateral_context,\n\u001b[1;32m    437\u001b[0m         allow_records,\n\u001b[1;32m    438\u001b[0m         broadcast_parameters_rule,\n\u001b[1;32m    439\u001b[0m         left_broadcast,\n\u001b[1;32m    440\u001b[0m         right_broadcast,\n\u001b[1;32m    441\u001b[0m         numpy_to_regular,\n\u001b[1;32m    442\u001b[0m         regular_to_jagged,\n\u001b[1;32m    443\u001b[0m         return_value,\n\u001b[1;32m    444\u001b[0m         behavior,\n\u001b[1;32m    445\u001b[0m         highlevel,\n\u001b[1;32m    446\u001b[0m     )\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/awkward/operations/ak_transform.py:508\u001b[0m, in \u001b[0;36m_impl\u001b[0;34m(transformation, array, more_arrays, depth_context, lateral_context, allow_records, broadcast_parameters_rule, left_broadcast, right_broadcast, numpy_to_regular, regular_to_jagged, return_value, behavior, highlevel)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[39m# An exception to the rule of ak._do.recursively_apply, for symmetry with\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[39m# ak._broadcasting.apply_step, below. ak_transform._impl knows implementation details.\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m out \u001b[39m=\u001b[39m layout\u001b[39m.\u001b[39;49m_recursively_apply(\n\u001b[1;32m    509\u001b[0m     action,\n\u001b[1;32m    510\u001b[0m     behavior,\n\u001b[1;32m    511\u001b[0m     \u001b[39m1\u001b[39;49m,\n\u001b[1;32m    512\u001b[0m     copy\u001b[39m.\u001b[39;49mcopy(depth_context),\n\u001b[1;32m    513\u001b[0m     lateral_context,\n\u001b[1;32m    514\u001b[0m     options,\n\u001b[1;32m    515\u001b[0m )\n\u001b[1;32m    517\u001b[0m \u001b[39mif\u001b[39;00m return_value \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/awkward/contents/listoffsetarray.py:2105\u001b[0m, in \u001b[0;36mListOffsetArray._recursively_apply\u001b[0;34m(self, action, behavior, depth, depth_context, lateral_context, options)\u001b[0m\n\u001b[1;32m   2104\u001b[0m \u001b[39melif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2105\u001b[0m     \u001b[39mreturn\u001b[39;00m continuation()\n\u001b[1;32m   2106\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/awkward/contents/listoffsetarray.py:2066\u001b[0m, in \u001b[0;36mListOffsetArray._recursively_apply.<locals>.continuation\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2065\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcontinuation\u001b[39m():\n\u001b[0;32m-> 2066\u001b[0m     \u001b[39mreturn\u001b[39;00m ListOffsetArray(\n\u001b[1;32m   2067\u001b[0m         offsets,\n\u001b[1;32m   2068\u001b[0m         content\u001b[39m.\u001b[39;49m_recursively_apply(\n\u001b[1;32m   2069\u001b[0m             action,\n\u001b[1;32m   2070\u001b[0m             behavior,\n\u001b[1;32m   2071\u001b[0m             depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2072\u001b[0m             copy\u001b[39m.\u001b[39;49mcopy(depth_context),\n\u001b[1;32m   2073\u001b[0m             lateral_context,\n\u001b[1;32m   2074\u001b[0m             options,\n\u001b[1;32m   2075\u001b[0m         ),\n\u001b[1;32m   2076\u001b[0m         parameters\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parameters \u001b[39mif\u001b[39;49;00m options[\u001b[39m\"\u001b[39;49m\u001b[39mkeep_parameters\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2077\u001b[0m     )\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/awkward/contents/listoffsetarray.py:139\u001b[0m, in \u001b[0;36mListOffsetArray.__init__\u001b[0;34m(self, offsets, content, parameters)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is a bytestring, so its \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be uint8 NumpyArray of byte, not \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    135\u001b[0m                 \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39mrepr\u001b[39m(content)\n\u001b[1;32m    136\u001b[0m             )\n\u001b[1;32m    137\u001b[0m         )\n\u001b[0;32m--> 139\u001b[0m \u001b[39massert\u001b[39;00m offsets\u001b[39m.\u001b[39mnplike \u001b[39mis\u001b[39;00m content\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mindex_nplike\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offsets \u001b[39m=\u001b[39m offsets\n",
      "\u001b[0;31mAssertionError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m dak_events \u001b[39m=\u001b[39m open_events(permit_dask\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m dak_results \u001b[39m=\u001b[39m pn_example1(dak_events)\n\u001b[0;32m---> 62\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDask awkward results:\u001b[39m\u001b[39m\"\u001b[39m, dak_results\u001b[39m.\u001b[39;49mcompute())  \u001b[39m# Also runs file!\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m# Checking that the results are identical\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39massert\u001b[39;00m ak\u001b[39m.\u001b[39mall(dak_results\u001b[39m.\u001b[39mcompute() \u001b[39m==\u001b[39m ak_results)\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    291\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m    dask.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    600\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         _execute_task(task, data)  \u001b[39m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[39m=\u001b[39m loads(res_info)\n\u001b[1;32m    513\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m][key] \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/dask/local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m exc\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[1;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 319\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[39m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, data)\n\u001b[1;32m    225\u001b[0m     \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[39m=\u001b[39m dumps((result, \u001b[39mid\u001b[39m))\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m(_execute_task(a, cache) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/dask/optimization.py:990\u001b[0m, in \u001b[0;36mSubgraphCallable.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minkeys):\n\u001b[1;32m    989\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m args, got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minkeys), \u001b[39mlen\u001b[39m(args)))\n\u001b[0;32m--> 990\u001b[0m \u001b[39mreturn\u001b[39;00m core\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdsk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutkey, \u001b[39mdict\u001b[39;49m(\u001b[39mzip\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minkeys, args)))\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/dask/core.py:149\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, out, cache)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m toposort(dsk):\n\u001b[1;32m    148\u001b[0m     task \u001b[39m=\u001b[39m dsk[key]\n\u001b[0;32m--> 149\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, cache)\n\u001b[1;32m    150\u001b[0m     cache[key] \u001b[39m=\u001b[39m result\n\u001b[1;32m    151\u001b[0m result \u001b[39m=\u001b[39m _execute_task(out, cache)\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m(_execute_task(a, cache) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/coffea/ml_tools/helper.py:325\u001b[0m, in \u001b[0;36mnumpy_call_wrapper._call_dask.<locals>._callable_wrap.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    321\u001b[0m eval_args, eval_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs_to_pair(\u001b[39m*\u001b[39m\u001b[39mtuple\u001b[39m(v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m args))\n\u001b[1;32m    323\u001b[0m \u001b[39m# awkward.zip so that the return is a single awkward\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[39m# array\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrapper\u001b[39m.\u001b[39;49m_call_awkward(\u001b[39m*\u001b[39;49meval_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49meval_kwargs)\n\u001b[1;32m    326\u001b[0m out \u001b[39m=\u001b[39m pack_ret_array(out)\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_backend(\u001b[39m*\u001b[39margs) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtypetracer\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/coffea/ml_tools/helper.py:244\u001b[0m, in \u001b[0;36mnumpy_call_wrapper._call_awkward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_awkward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    240\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39m    The common routine of awkward_to_numpy conversion, numpy evaluation,\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39m    then numpy_to_awkward conversion.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m     np_args, np_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mawkward_to_numpy(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    245\u001b[0m     np_rets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_numpy(\u001b[39m*\u001b[39mnp_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnp_kwargs)\n\u001b[1;32m    246\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy_to_awkward(np_rets, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/coffea/ml_tools/helper.py:213\u001b[0m, in \u001b[0;36mnumpy_call_wrapper.awkward_to_numpy\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mawkward_to_numpy\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple:\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m    Function called to actually convert awkward arrays to numpy arrays.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m    Conversion is handled by the typetracer variant of awkward.to_numpy so\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m    that the relevant columns are automatically detected and handled when\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39m    using dask_awkward arrays.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     np_args, np_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_awkward_to_numpy(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mtypetrace_converter\u001b[39m(arg):\n\u001b[1;32m    216\u001b[0m         \u001b[39mreturn\u001b[39;00m awkward\u001b[39m.\u001b[39mtypetracer\u001b[39m.\u001b[39mlength_one_if_typetracer(arg)\u001b[39m.\u001b[39mto_numpy()\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mParticleNetExample1.prepare_awkward_to_numpy\u001b[0;34m(self, events)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m ak\u001b[39m.\u001b[39mfill_none(\n\u001b[1;32m     12\u001b[0m         ak\u001b[39m.\u001b[39mpad_none(arr, \u001b[39m100\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, clip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     13\u001b[0m         \u001b[39m0.0\u001b[39m,\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m \u001b[39m# Human readable version of what the inputs are\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# Each array is a N jets x 100 constituent array\u001b[39;00m\n\u001b[1;32m     18\u001b[0m imap \u001b[39m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpoints\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m---> 20\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdeta\u001b[39m\u001b[39m\"\u001b[39m: pad(jets\u001b[39m.\u001b[39meta \u001b[39m-\u001b[39m jets\u001b[39m.\u001b[39;49mconstituents\u001b[39m.\u001b[39mpf\u001b[39m.\u001b[39meta),\n\u001b[1;32m     21\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdphi\u001b[39m\u001b[39m\"\u001b[39m: pad(jets\u001b[39m.\u001b[39mdelta_phi(jets\u001b[39m.\u001b[39mconstituents\u001b[39m.\u001b[39mpf)),\n\u001b[1;32m     22\u001b[0m     },\n\u001b[1;32m     23\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m     24\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdr\u001b[39m\u001b[39m\"\u001b[39m: pad(jets\u001b[39m.\u001b[39mdelta_r(jets\u001b[39m.\u001b[39mconstituents\u001b[39m.\u001b[39mpf)),\n\u001b[1;32m     25\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlpt\u001b[39m\u001b[39m\"\u001b[39m: pad(np\u001b[39m.\u001b[39mlog(jets\u001b[39m.\u001b[39mconstituents\u001b[39m.\u001b[39mpf\u001b[39m.\u001b[39mpt)),\n\u001b[1;32m     26\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlptf\u001b[39m\u001b[39m\"\u001b[39m: pad(np\u001b[39m.\u001b[39mlog(jets\u001b[39m.\u001b[39mconstituents\u001b[39m.\u001b[39mpf\u001b[39m.\u001b[39mpt \u001b[39m/\u001b[39m jets\u001b[39m.\u001b[39mpt)),\n\u001b[1;32m     27\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m: pad(np\u001b[39m.\u001b[39mlog(np\u001b[39m.\u001b[39mabs(jets\u001b[39m.\u001b[39mconstituents\u001b[39m.\u001b[39mpf\u001b[39m.\u001b[39md0) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)),\n\u001b[1;32m     28\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mf2\u001b[39m\u001b[39m\"\u001b[39m: pad(np\u001b[39m.\u001b[39mlog(np\u001b[39m.\u001b[39mabs(jets\u001b[39m.\u001b[39mconstituents\u001b[39m.\u001b[39mpf\u001b[39m.\u001b[39mdz) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)),\n\u001b[1;32m     29\u001b[0m     },\n\u001b[1;32m     30\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m     31\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m\"\u001b[39m: pad(ak\u001b[39m.\u001b[39mones_like(jets\u001b[39m.\u001b[39mconstituents\u001b[39m.\u001b[39mpf\u001b[39m.\u001b[39mpt)),\n\u001b[1;32m     32\u001b[0m     },\n\u001b[1;32m     33\u001b[0m }\n\u001b[1;32m     35\u001b[0m \u001b[39m# Compacting the array elements into the desired dimension using\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# ak.concatenate\u001b[39;00m\n\u001b[1;32m     37\u001b[0m retmap \u001b[39m=\u001b[39m {\n\u001b[1;32m     38\u001b[0m     k: ak\u001b[39m.\u001b[39mconcatenate([x[:, np\u001b[39m.\u001b[39mnewaxis, :] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m imap[k]\u001b[39m.\u001b[39mvalues()], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m imap\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m     40\u001b[0m }\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/coffea/nanoevents/methods/nanoaod.py:424\u001b[0m, in \u001b[0;36mJet.constituents\u001b[0;34m(self, _dask_array_)\u001b[0m\n\u001b[1;32m    420\u001b[0m     original \u001b[39m=\u001b[39m _dask_array_\u001b[39m.\u001b[39mbehavior[\u001b[39m\"\u001b[39m\u001b[39m__original_array__\u001b[39m\u001b[39m\"\u001b[39m]()\u001b[39m.\u001b[39mJetPFCands\n\u001b[1;32m    421\u001b[0m     \u001b[39mreturn\u001b[39;00m original\u001b[39m.\u001b[39m_apply_global_index(\n\u001b[1;32m    422\u001b[0m         _dask_array_\u001b[39m.\u001b[39mpFCandsIdxG, _dask_array_\u001b[39m=\u001b[39moriginal\n\u001b[1;32m    423\u001b[0m     )\n\u001b[0;32m--> 424\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_events()\u001b[39m.\u001b[39;49mJetPFCands\u001b[39m.\u001b[39;49m_apply_global_index(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpFCandsIdxG)\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/dask_awkward/lib/core.py:1060\u001b[0m, in \u001b[0;36mArray.__getattr__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1060\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_behavior_method(attr, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/dask_awkward/lib/core.py:987\u001b[0m, in \u001b[0;36mArray._call_behavior_method\u001b[0;34m(self, method_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_dask_array_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    986\u001b[0m             kwargs[\u001b[39m\"\u001b[39m\u001b[39m_dask_array_\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m--> 987\u001b[0m         \u001b[39mreturn\u001b[39;00m themethod(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    988\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_partitions(\n\u001b[1;32m    989\u001b[0m         _BehaviorMethodFn(method_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs),\n\u001b[1;32m    990\u001b[0m         \u001b[39m*\u001b[39margs,\n\u001b[1;32m    991\u001b[0m         label\u001b[39m=\u001b[39mhyphenize(method_name),\n\u001b[1;32m    992\u001b[0m     )\n\u001b[1;32m    994\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    995\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMethod \u001b[39m\u001b[39m{\u001b[39;00mmethod_name\u001b[39m}\u001b[39;00m\u001b[39m is not available to this collection.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    996\u001b[0m )\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/coffea/nanoevents/methods/base.py:226\u001b[0m, in \u001b[0;36mNanoCollection._apply_global_index\u001b[0;34m(self, index, _dask_array_)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mreturn\u001b[39;00m flat_take(layout)\n\u001b[1;32m    223\u001b[0m (index_out,) \u001b[39m=\u001b[39m awkward\u001b[39m.\u001b[39mbroadcast_arrays(\n\u001b[1;32m    224\u001b[0m     index\u001b[39m.\u001b[39m_meta \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(index, dask_awkward\u001b[39m.\u001b[39mArray) \u001b[39melse\u001b[39;00m index\n\u001b[1;32m    225\u001b[0m )\n\u001b[0;32m--> 226\u001b[0m layout_out \u001b[39m=\u001b[39m awkward\u001b[39m.\u001b[39;49mtransform(descend, index_out\u001b[39m.\u001b[39;49mlayout, highlevel\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    227\u001b[0m out \u001b[39m=\u001b[39m awkward\u001b[39m.\u001b[39mArray(layout_out, behavior\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbehavior)\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(index, dask_awkward\u001b[39m.\u001b[39mArray):\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/awkward/operations/ak_transform.py:431\u001b[0m, in \u001b[0;36mtransform\u001b[0;34m(transformation, array, depth_context, lateral_context, allow_records, broadcast_parameters_rule, left_broadcast, right_broadcast, numpy_to_regular, regular_to_jagged, return_value, highlevel, behavior, *more_arrays)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m    transformation (callable): Function to apply to each node of the array.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39moutputs.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[39mwith\u001b[39;00m ak\u001b[39m.\u001b[39m_errors\u001b[39m.\u001b[39mOperationErrorContext(\n\u001b[1;32m    413\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mak.transform\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    414\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m     },\n\u001b[1;32m    430\u001b[0m ):\n\u001b[0;32m--> 431\u001b[0m     \u001b[39mreturn\u001b[39;00m _impl(\n\u001b[1;32m    432\u001b[0m         transformation,\n\u001b[1;32m    433\u001b[0m         array,\n\u001b[1;32m    434\u001b[0m         more_arrays,\n\u001b[1;32m    435\u001b[0m         depth_context,\n\u001b[1;32m    436\u001b[0m         lateral_context,\n\u001b[1;32m    437\u001b[0m         allow_records,\n\u001b[1;32m    438\u001b[0m         broadcast_parameters_rule,\n\u001b[1;32m    439\u001b[0m         left_broadcast,\n\u001b[1;32m    440\u001b[0m         right_broadcast,\n\u001b[1;32m    441\u001b[0m         numpy_to_regular,\n\u001b[1;32m    442\u001b[0m         regular_to_jagged,\n\u001b[1;32m    443\u001b[0m         return_value,\n\u001b[1;32m    444\u001b[0m         behavior,\n\u001b[1;32m    445\u001b[0m         highlevel,\n\u001b[1;32m    446\u001b[0m     )\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/awkward/_errors.py:56\u001b[0m, in \u001b[0;36mErrorContext.__exit__\u001b[0;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[39m# Handle caught exception\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[39mif\u001b[39;00m exception_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprimary() \u001b[39mis\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m---> 56\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_exception(exception_type, exception_value)\n\u001b[1;32m     57\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[39m# `_kwargs` may hold cyclic references, that we really want to avoid\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[39m# as this can lead to large buffers remaining in memory for longer than absolutely necessary\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# Let's just clear this, now.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kwargs\u001b[39m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/VirtualENV/coffea-test/lib/python3.8/site-packages/awkward/_errors.py:71\u001b[0m, in \u001b[0;36mErrorContext.handle_exception\u001b[0;34m(self, cls, exception)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecorate_exception(\u001b[39mcls\u001b[39m, exception)\n\u001b[1;32m     70\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecorate_exception(\u001b[39mcls\u001b[39m, exception)\n",
      "\u001b[0;31mAssertionError\u001b[0m: \n\nSee if this has been reported at https://github.com/scikit-hep/awkward/issues"
     ]
    }
   ],
   "source": [
    "from coffea.ml_tools.torch_wrapper import torch_wrapper\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ParticleNetExample1(torch_wrapper):\n",
    "    def prepare_awkward_to_numpy(self, events):\n",
    "        jets = ak.flatten(events.Jet)\n",
    "\n",
    "        def pad(arr):\n",
    "            return ak.fill_none(\n",
    "                ak.pad_none(arr, 100, axis=1, clip=True),\n",
    "                0.0,\n",
    "            )\n",
    "\n",
    "        # Human readable version of what the inputs are\n",
    "        # Each array is a N jets x 100 constituent array\n",
    "        imap = {\n",
    "            \"points\": {\n",
    "                \"deta\": pad(jets.eta - jets.constituents.pf.eta),\n",
    "                \"dphi\": pad(jets.delta_phi(jets.constituents.pf)),\n",
    "            },\n",
    "            \"features\": {\n",
    "                \"dr\": pad(jets.delta_r(jets.constituents.pf)),\n",
    "                \"lpt\": pad(np.log(jets.constituents.pf.pt)),\n",
    "                \"lptf\": pad(np.log(jets.constituents.pf.pt / jets.pt)),\n",
    "                \"f1\": pad(np.log(np.abs(jets.constituents.pf.d0) + 1)),\n",
    "                \"f2\": pad(np.log(np.abs(jets.constituents.pf.dz) + 1)),\n",
    "            },\n",
    "            \"mask\": {\n",
    "                \"mask\": pad(ak.ones_like(jets.constituents.pf.pt)),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Compacting the array elements into the desired dimension using\n",
    "        # ak.concatenate\n",
    "        retmap = {\n",
    "            k: ak.concatenate([x[:, np.newaxis, :] for x in imap[k].values()], axis=1)\n",
    "            for k in imap.keys()\n",
    "        }\n",
    "\n",
    "        # Returning everything using a dictionary. Also take care of type\n",
    "        # conversion here.\n",
    "        return (), {\n",
    "            \"points\": ak.values_astype(retmap[\"points\"], \"float32\"),\n",
    "            \"features\": ak.values_astype(retmap[\"features\"], \"float32\"),\n",
    "            \"mask\": ak.values_astype(retmap[\"mask\"], \"float16\"),\n",
    "        }\n",
    "\n",
    "\n",
    "# Setting up the model container\n",
    "pn_example1 = ParticleNetExample1(\"model.pt\")\n",
    "\n",
    "# Running on awkward arrays\n",
    "# ak_events = open_events(permit_dask=False)\n",
    "# ak_results = pn_example1(ak_events)\n",
    "# print(\"Awkward results:\", ak_results)  # Runs fine!\n",
    "\n",
    "# Running on dask_awkward array\n",
    "dak_events = open_events(permit_dask=True)\n",
    "dak_results = pn_example1(dak_events)\n",
    "print(\"Dask awkward results:\", dak_results.compute())  # Also runs file!\n",
    "\n",
    "# Checking that the results are identical\n",
    "assert ak.all(dak_results.compute() == ak_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each jet in the input to the `torch` model, the model returns a 2-tuple\n",
    "probability value. Without additional specification, the `torch_wrapper` class\n",
    "performs a trival conversion of `ak.from_numpy` of the torch model's output. We\n",
    "can specify that we want to fold this back into nested structure by overloading\n",
    "the `numpy_to_awkward` method of the class. \n",
    "\n",
    "For the ParticleNet example we are going perform additional computation for the\n",
    "conversion back to awkward array formats: \n",
    "\n",
    "- Calculate the `softmax` method for the return of each jet (commonly used as\n",
    "  the singular ML inference \"scores\")\n",
    "- Fold the computed `softmax` array back into nested structure that is\n",
    "  compatible with the original events.Jet array.\n",
    "\n",
    "Notice that the inputs of the `numpy_to_awkward` method is different from the\n",
    "`prepare_awkward_to_numpy` method, only by that the first argument is the return\n",
    "`numpy` array of the model inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ensc/VirtualENV/coffea-test/lib/python3.8/site-packages/coffea/ml_tools/helper.py:155: UserWarning: No format checks were performed on input!\n",
      "  warnings.warn(\"No format checks were performed on input!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.68e+03, 1.41e+03, 298, 85.9, 32.7, 16.3, 16.2, 15.8], ..., [1.58e+03, ...]]\n",
      "[[0.528, 0.528, 0.524, 0.523, 0.521, 0.52, 0.519, 0.519], ..., [0.528, ...]]\n"
     ]
    }
   ],
   "source": [
    "class ParticleNetExample2(ParticleNetExample1):\n",
    "    def numpy_to_awkward(self, return_array, events):\n",
    "        softmax = np.exp(return_array)[:, 0] / np.sum(np.exp(return_array), axis=-1)\n",
    "    \n",
    "        njets = ak.count(\n",
    "            ak.typetracer.length_one_if_typetracer(events.Jet.pt), \n",
    "            axis=-1\n",
    "        )\n",
    "        if ak.backend(events) == \"typetracer\":\n",
    "            njets = ak.full_like(njets, 1)\n",
    "        out = ak.unflatten(softmax, njets)\n",
    "        if ak.backend(events) == \"typetracer\":\n",
    "            out = ak.Array(\n",
    "                out.layout.to_typetracer(forget_length=True), \n",
    "                behavior=out.behavior\n",
    "            )\n",
    "        return out\n",
    "\n",
    "\n",
    "pn_example2 = ParticleNetExample2(\"model.pt\")\n",
    "jets = events.Jet\n",
    "jets[\"MLresults\"] = pn_example2(events)\n",
    "events[\"Jet\"] = jets\n",
    "\n",
    "print(events.Jet.pt)\n",
    "print(events.Jet.MLresults)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.68e+03, 1.41e+03, 298, 85.9, 32.7, 16.3, 16.2, 15.8], ..., [1.58e+03, ...]]\n",
      "[[0.528, 0.528, 0.524, 0.523, 0.521, 0.52, 0.519, 0.519], ..., [0.528, ...]]\n"
     ]
    }
   ],
   "source": [
    "class ParticleNetExample3(ParticleNetExample1):\n",
    "    def numpy_to_awkward(self, return_array, events):\n",
    "        softmax = np.exp(return_array)[:, 0] / np.sum(np.exp(return_array), axis=-1)\n",
    "    \n",
    "        njets = ak.count(\n",
    "            ak.typetracer.length_one_if_typetracer(events.Jet.pt), \n",
    "            axis=-1\n",
    "        )\n",
    "        if ak.backend(events) == \"typetracer\":\n",
    "            njets = ak.full_like(njets, 1)\n",
    "        out = ak.unflatten(softmax, njets)\n",
    "        if ak.backend(events) == \"typetracer\":\n",
    "            out = ak.Array(\n",
    "                out.layout.to_typetracer(forget_length=True), \n",
    "                behavior=out.behavior\n",
    "            )\n",
    "        return out\n",
    "\n",
    "\n",
    "pn_example2 = ParticleNetExample2(\"model.pt\")\n",
    "jets = events.Jet\n",
    "jets[\"MLresults\"] = pn_example2(events)\n",
    "events[\"Jet\"] = jets\n",
    "\n",
    "print(events.Jet.pt)\n",
    "print(events.Jet.MLresults)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a per-jet variable we can use to continue the event/object selection\n",
    "chain for our analysis! \n",
    "\n",
    "Notice that up till now, we have been working exclusively with plain awkward.\n",
    "But this class is already ready to be extended to dask awkward! The `__call__`\n",
    "method commonly knows how to handle the different array types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.528, 0.528, 0.524, 0.523, 0.521, 0.52, 0.519, 0.519], ..., [0.528, ...]]\n",
      "True\n",
      "{'from-uproot-56eb4fdb675ad421bf8d8d2de4d060d4': ['Jet.phi', 'PFCands.eta', 'Jet.eta', 'PFCands.dz', 'JetPFCands.pFCandsIdx', 'PFCands.d0', 'PFCands.phi', 'Jet.pt', 'PFCands.pt', 'Jet.nConstituents']}\n"
     ]
    }
   ],
   "source": [
    "import dask_awkward as dak\n",
    "\n",
    "# Creating a lazy dask array of events array with the same\n",
    "dask_events = open_events(permit_dask=True)\n",
    "\n",
    "# Syntax for dask arrays is identical to the plain awkward arrays!\n",
    "dask_jets = dask_events.Jet\n",
    "dask_jets[\"MLresults_dask\"] = pn_example2(dask_events)\n",
    "dask_events[\"Jet\"] = dask_jets\n",
    "\n",
    "# Checking that we get identical results\n",
    "print(dask_events.Jet.MLresults_dask.compute())\n",
    "print(ak.all(dask_events.Jet.MLresults_dask.compute() == events.Jet.MLresults))\n",
    "\n",
    "# Check which columns are loaded\n",
    "print(dak.necessary_columns(dask_events.Jet.MLresults_dask))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the implementation of the classes above can be written in a single\n",
    "class. Here is a copy-and-paste implementation of the class with all the\n",
    "functionality described in the cells above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.528, 0.528, 0.524, 0.523, 0.521, 0.52, 0.519, 0.519], ..., [0.528, ...]]\n",
      "{'from-uproot-677376b5bab4d79c2a28f905153d2532': ['Jet.phi', 'PFCands.eta', 'Jet.eta', 'PFCands.dz', 'JetPFCands.pFCandsIdx', 'PFCands.d0', 'PFCands.phi', 'Jet.pt', 'PFCands.pt', 'Jet.nConstituents']}\n"
     ]
    }
   ],
   "source": [
    "class ParticleNetExample(torch_wrapper):\n",
    "    def prepare_awkward_to_numpy(self, events):\n",
    "        jets = ak.flatten(events.Jet)\n",
    "\n",
    "        def pad(arr):\n",
    "            return ak.fill_none(\n",
    "                ak.pad_none(arr, 100, axis=1, clip=True),\n",
    "                0.0,\n",
    "            )\n",
    "\n",
    "        # Human readable version of what the inputs are\n",
    "        # Each array is a N jets x 100 constituent array\n",
    "        imap = {\n",
    "            \"points\": {\n",
    "                \"deta\": pad(jets.eta - jets.PFCands.eta),\n",
    "                \"dphi\": pad(jets.delta_phi(jets.PFCands)),\n",
    "            },\n",
    "            \"features\": {\n",
    "                \"dr\": pad(jets.delta_r(jets.PFCands)),\n",
    "                \"lpt\": pad(np.log(jets.PFCands.pt)),\n",
    "                \"lptf\": pad(np.log(jets.PFCands.pt / jets.pt)),\n",
    "                \"f1\": pad(np.log(np.abs(jets.PFCands.d0) + 1)),\n",
    "                \"f2\": pad(np.log(np.abs(jets.PFCands.dz) + 1)),\n",
    "            },\n",
    "            \"mask\": {\n",
    "                \"mask\": pad(ak.ones_like(jets.PFCands.pt)),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Compacting the array elements into the desired dimension using\n",
    "        # ak.concatenate\n",
    "        retmap = {\n",
    "            k: ak.concatenate([x[:, np.newaxis, :] for x in imap[k].values()], axis=1)\n",
    "            for k in imap.keys()\n",
    "        }\n",
    "\n",
    "        # Returning everything using a dictionary. Also take care of type\n",
    "        # conversion here.\n",
    "        return (), {\n",
    "            \"points\": ak.values_astype(retmap[\"points\"], \"float32\"),\n",
    "            \"features\": ak.values_astype(retmap[\"features\"], \"float32\"),\n",
    "            \"mask\": ak.values_astype(retmap[\"mask\"], \"float16\"),\n",
    "        }\n",
    "\n",
    "    def numpy_to_awkward(self, return_array, events):\n",
    "        softmax = np.exp(return_array)[:, 0] / np.sum(np.exp(return_array), axis=-1)\n",
    "\n",
    "        njets = ak.count(\n",
    "            ak.typetracer.length_one_if_typetracer(events.Jet.pt), \n",
    "            axis=-1\n",
    "        )\n",
    "        if ak.backend(events) == \"typetracer\":\n",
    "            njets = ak.full_like(njets, 1)\n",
    "        out = ak.unflatten(softmax, njets)\n",
    "        if ak.backend(events) == \"typetracer\":\n",
    "            out = ak.Array(\n",
    "                out.layout.to_typetracer(forget_length=True), \n",
    "                behavior=out.behavior\n",
    "            )\n",
    "        return ak.unflatten(softmax, njets)\n",
    "\n",
    "\n",
    "\n",
    "pn_example = ParticleNetExample(\"model.pt\")\n",
    "\n",
    "# Reloading a lazy instance of the array\n",
    "dask_events = open_events(permit_dask=True)\n",
    "\n",
    "# Syntax for dask arrays is identical to the plain awkward arrays!\n",
    "dask_jets = dask_events.Jet\n",
    "dask_jets[\"MLresults\"] = pn_example(dask_events)\n",
    "dask_events[\"Jet\"] = dask_jets\n",
    "\n",
    "# Checking that we get identical results\n",
    "print(dask_events.Jet.MLresults.compute())\n",
    "\n",
    "# Check which columns are loaded\n",
    "print(dak.necessary_columns(dask_events.Jet.MLresults))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments about generalizing to other ML tools\n",
    "\n",
    "All ML wrappers provided in the `coffea.mltools` module (`triton_wrapper` for\n",
    "[triton][triton] server inference, `torch_wrapper` for pytorch, and\n",
    "`xgboost_wrapper` for [xgboost][xgboost] inference) follow the same design:\n",
    "analyzers is responsible for providing the model of interest, along with\n",
    "providing an inherited class that overloads of the following methods to data\n",
    "type conversion:\n",
    "\n",
    "- `awkward_to_numpy`: converting awkward arrays to `numpy` arrays, the output\n",
    "  `numpy` arrays should be in the format of a tuple `a` and a dictionary `b`,\n",
    "  which can be expanded out to the input of the ML tool like `model(*a, **b)`.\n",
    "  Notice some additional trivial conversion (like converting to available\n",
    "  kernels for `pytorch`, converting to a matrix format for `xgboost`, and slice\n",
    "  of array for `triton` is handled automatically by the respective wrappers)\n",
    "- `numpy_to_awkward` (optional): converting the number results back to awkward\n",
    "  array format. If this is not provided, then a simple `ak.from_numpy`\n",
    "  conversion takes place.\n",
    "- `dask_columns` (optional but recommended): Given the inputs to the\n",
    "  `awkward_to_numpy` method, list the branches required for the inference\n",
    "  calculation. If not provided, it will attempt to load all branches\n",
    "  recursively, which may have significant performance penalties.\n",
    "\n",
    "If the ML tool of choice for your analysis has not been implemented by the\n",
    "`coffea.mltools` modules, consider constructing your own with the provided\n",
    "`numpy_call_wrapper` base class in `coffea.mltools`. Aside from the functions\n",
    "listed above, you will also need to provide the `numpy_call` method to perform\n",
    "any additional data format conversions, and call the ML tool of choice. If you\n",
    "think your implementation is general, also consider submitting a PR to the\n",
    "`coffea` repository!\n",
    "\n",
    "[triton]: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver\n",
    "[xgboost]: https://xgboost.readthedocs.io/en/stable/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffea-developement",
   "language": "python",
   "name": "coffea-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
