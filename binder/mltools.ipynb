{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running inference tools\n",
    "\n",
    "As machine learning (ML) becomes more popular in HEP analysis, `coffea` also\n",
    "provide tools to assist with using ML tools within the coffea framework. For\n",
    "training and validation, you would likely need custom data mangling tools to\n",
    "convert HEP data formats ([NanoAOD][nanoaod], [PFNano][pfnano]) to a format that\n",
    "best interfaces with the ML tool of choice, as for training and validation, you\n",
    "typical want to have fine control over what computation is done. For more\n",
    "advanced use cases of data mangling and data saving, refer to the [awkward array\n",
    "manual][datamangle] and [uproot][uproot_write]/[parquet][ak_parquet] write\n",
    "operations for saving intermediate states. The helper tools provided in coffea\n",
    "focuses on ML inference, where ML tool outputs are used as another variable to\n",
    "be used in the event/object selection chain.\n",
    "\n",
    "[nanoaod]: https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookNanoAOD\n",
    "[pfnano]: https://github.com/cms-jet/PFNano\n",
    "[datamangle]: https://awkward-array.org/doc/main/user-guide/how-to-restructure.html\n",
    "[uproot_write]: https://uproot.readthedocs.io/en/latest/basic.html#writing-ttrees-to-a-file\n",
    "[ak_parquet]: https://awkward-array.org/doc/main/reference/generated/ak.to_parquet.html\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why these wrapper tools are needed\n",
    "\n",
    "The typical operation of using ML inference tools in the awkward/coffea analysis\n",
    "tools involves the conversion and padding of awkward array to ML tool containers\n",
    "(usually something that is `numpy`-compatible), run the inference, then\n",
    "convert-and-truncate back into the awkward array syntax required for the\n",
    "analysis chain to continue. With awkward arrays' laziness now being handled\n",
    "entirely by [`dask`][dask_awkward], the conversion operation of awkward array to\n",
    "other array types needs to be wrapped in a way that is understandable to `dask`.\n",
    "The packages in the `ml_tools` package attempts to wrap the common tools used by\n",
    "the HEP community with a common interface to reduce the verbosity of the code on\n",
    "the analysis side.\n",
    "\n",
    "[dask_awkward]: https://dask-awkward.readthedocs.io/en/stable/gs-limitations.html\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using ParticleNet-like jet variable calculation using PyTorch\n",
    "\n",
    "The example given in this notebook be using [`pytorch`][pytorch] to calculate a\n",
    "jet-level discriminant using its constituent particles. An example for how to\n",
    "construct such a `pytorch` network can be found in the docs file, but for\n",
    "`mltools` in coffea, we only support the [TorchScript][pytorch] format files to\n",
    "load models to ensure operability when scaling to clusters. Let us first start\n",
    "by downloading the example ParticleNet model file and a small `PFNano`\n",
    "compatible file, and a simple function to open the `PFNano` with and without\n",
    "dask.\n",
    "\n",
    "\n",
    "[pytorch]: https://pytorch.org/\n",
    "[pytorch_jit]: https://pytorch.org/tutorials/beginner/saving_loading_models.html#export-load-model-in-torchscript-format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --quiet -O model.pt https://github.com/CoffeaTeam/coffea/raw/ml_tools/tests/samples/triton_models_test/pn_test/1/model.pt\n",
    "\n",
    "!wget --quiet -O pfnano.root https://github.com/yimuchen/coffea/raw/ml_tools/tests/samples/pfnano.root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.nanoevents import NanoEventsFactory\n",
    "from coffea.nanoevents.schemas import PFNanoAODSchema\n",
    "\n",
    "\n",
    "def open_events(permit_dask=False):\n",
    "    factory = NanoEventsFactory.from_root(\n",
    "        \"file:./pfnano.root\",\n",
    "        schemaclass=PFNanoAODSchema,\n",
    "        permit_dask=permit_dask,\n",
    "    )\n",
    "    return factory.events()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare a class to handle inference request by extending the\n",
    "`mltools.torch_wrapper` class. As the base class cannot know anything about the\n",
    "data mangling required for the users particular model, we will need to overload\n",
    "at least the method `prepare_awkward_to_numpy`:\n",
    "\n",
    "- The input can be an arbitrary number of awkward arrays. In this example, we\n",
    "  will be passing in the event array.\n",
    "- The output should be single tuple `a` + single dictionary `b`, this is to\n",
    "  ensure that arbitrarily complicated outputs can be passed to the underlying\n",
    "  `pytorch` model instance like `model(*a, **b)`. The contents of `a` and `b`\n",
    "  should be `numpy`-compatible _awkward_ arrays (so awkward arrays that can be\n",
    "  trivially converted to `numpy` arrays via a `ak.to_numpy` call). In this\n",
    "  ParticleNet-like example, the model expects the following inputs:\n",
    "\n",
    "  - A `N` jets x `2` coordinate x `100` constituents \"points\" array,\n",
    "    representing the constituent coordinates.\n",
    "  - A `N` jets x `5` feature x `100` constituents \"features\" array, representing\n",
    "    the constituent features of interest to be used for inference.\n",
    "  - A `N` jets x `1` mask x `100` constituent \"mask\" array, representing whether\n",
    "    a constituent should be masked from the inference request.\n",
    "\n",
    "  In this case, we will need to flatten the `E` events x `N` jets structure, as\n",
    "  well as stack the constituent attributes of interest, into a single array.\n",
    "\n",
    "After defining this minimum class, we can attempt to run an inference using the\n",
    "`__call__` method defined in the base class. Notice that overloading this single\n",
    "method will automatically allow for the inference to be called on both awkward\n",
    "and dask-awkward. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.ml_tools.torch_wrapper import torch_wrapper\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ParticleNetExample1(torch_wrapper):\n",
    "    def prepare_awkward_to_numpy(self, events):\n",
    "        jets = ak.flatten(events.Jet)\n",
    "\n",
    "        def pad(arr):\n",
    "            return ak.fill_none(\n",
    "                ak.pad_none(arr, 100, axis=1, clip=True),\n",
    "                0.0,\n",
    "            )\n",
    "\n",
    "        # Human readable version of what the inputs are\n",
    "        # Each array is a N jets x 100 constituent array\n",
    "        imap = {\n",
    "            \"points\": {\n",
    "                \"deta\": pad(jets.eta - jets.constituents.pf.eta),\n",
    "                \"dphi\": pad(jets.delta_phi(jets.constituents.pf)),\n",
    "            },\n",
    "            \"features\": {\n",
    "                \"dr\": pad(jets.delta_r(jets.constituents.pf)),\n",
    "                \"lpt\": pad(np.log(jets.constituents.pf.pt)),\n",
    "                \"lptf\": pad(np.log(jets.constituents.pf.pt / jets.pt)),\n",
    "                \"f1\": pad(np.log(np.abs(jets.constituents.pf.d0) + 1)),\n",
    "                \"f2\": pad(np.log(np.abs(jets.constituents.pf.dz) + 1)),\n",
    "            },\n",
    "            \"mask\": {\n",
    "                \"mask\": pad(ak.ones_like(jets.constituents.pf.pt)),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Compacting the array elements into the desired dimension using\n",
    "        # ak.concatenate\n",
    "        retmap = {\n",
    "            k: ak.concatenate([x[:, np.newaxis, :] for x in imap[k].values()], axis=1)\n",
    "            for k in imap.keys()\n",
    "        }\n",
    "\n",
    "        # Returning everything using a dictionary. Also take care of type\n",
    "        # conversion here.\n",
    "        return (), {\n",
    "            \"points\": ak.values_astype(retmap[\"points\"], \"float32\"),\n",
    "            \"features\": ak.values_astype(retmap[\"features\"], \"float32\"),\n",
    "            \"mask\": ak.values_astype(retmap[\"mask\"], \"float16\"),\n",
    "        }\n",
    "\n",
    "\n",
    "# Setting up the model container\n",
    "pn_example1 = ParticleNetExample1(\"model.pt\")\n",
    "\n",
    "# Running on awkward arrays\n",
    "ak_events = open_events(permit_dask=False)\n",
    "ak_results = pn_example1(ak_events)\n",
    "print(\"Awkward results:\", ak_results)  # Runs fine!\n",
    "\n",
    "# Running on dask_awkward array\n",
    "dak_events = open_events(permit_dask=True)\n",
    "dak_results = pn_example1(dak_events)\n",
    "print(\"Dask awkward results:\", dak_results.compute())  # Also runs file!\n",
    "\n",
    "# Checking that the results are identical\n",
    "assert ak.all(dak_results.compute() == ak_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each jet in the input to the `torch` model, the model returns a 2-tuple\n",
    "probability value. Without additional specification, the `torch_wrapper` class\n",
    "performs a trival conversion of `ak.from_numpy` of the torch model's output. We\n",
    "can specify that we want to fold this back into nested structure by overloading\n",
    "the `numpy_to_awkward` method of the class. \n",
    "\n",
    "For the ParticleNet example we are going perform additional computation for the\n",
    "conversion back to awkward array formats: \n",
    "\n",
    "- Calculate the `softmax` method for the return of each jet (commonly used as\n",
    "  the singular ML inference \"scores\")\n",
    "- Fold the computed `softmax` array back into nested structure that is\n",
    "  compatible with the original events.Jet array.\n",
    "\n",
    "Notice that the inputs of the `numpy_to_awkward` method is different from the\n",
    "`prepare_awkward_to_numpy` method, only by that the first argument is the return\n",
    "`numpy` array of the model inference. If you overload this method, the\n",
    "appropriate supporting also needs to be exposed to dask for the best results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleNetExample2(ParticleNetExample1):\n",
    "    def numpy_to_awkward(self, return_array, events):\n",
    "        softmax = np.exp(return_array)[:, 0] / np.sum(np.exp(return_array), axis=-1)\n",
    "\n",
    "        njets = ak.count(ak.typetracer.length_one_if_typetracer(events.Jet.pt), axis=-1)\n",
    "        if ak.backend(events) == \"typetracer\":\n",
    "            njets = ak.full_like(njets, 1)\n",
    "        out = ak.unflatten(softmax, njets)\n",
    "        if ak.backend(events) == \"typetracer\":\n",
    "            out = ak.Array(\n",
    "                out.layout.to_typetracer(forget_length=True), behavior=out.behavior\n",
    "            )\n",
    "        return out\n",
    "\n",
    "\n",
    "pn_example2 = ParticleNetExample2(\"model.pt\")\n",
    "\n",
    "# Running on awkward\n",
    "ak_events = open_events(permit_dask=False)\n",
    "ak_jets = ak_events.Jet\n",
    "ak_jets[\"MLresults\"] = pn_example2(ak_events)\n",
    "ak_events[\"Jet\"] = ak_jets\n",
    "\n",
    "# Running on dask awkward\n",
    "dask_events = open_events(permit_dask=True)\n",
    "dask_jets = dask_events.Jet\n",
    "dask_jets[\"MLresults\"] = pn_example2(dask_events)\n",
    "dask_events[\"Jet\"] = dask_jets\n",
    "\n",
    "print(ak_events.Jet.MLresults)\n",
    "assert ak.all(ak_events.Jet.MLresults == dask_events.Jet.MLresults.compute())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the implementation of the classes above can be written in a single\n",
    "class. Here is a copy-and-paste implementation of the class with all the\n",
    "functionality described in the cells above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jet_features_as_numpy(jets):\n",
    "    def pad(arr):\n",
    "        return ak.fill_none(\n",
    "            ak.pad_none(arr, 100, axis=1, clip=True),\n",
    "            0.0,\n",
    "        )\n",
    "\n",
    "    # Human readable version of what the inputs are\n",
    "    # Each array is a N jets x 100 constituent array\n",
    "    imap = {\n",
    "        \"points\": {\n",
    "            \"deta\": pad(jets.eta - jets.PFCands.eta),\n",
    "            \"dphi\": pad(jets.delta_phi(jets.PFCands)),\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"dr\": pad(jets.delta_r(jets.PFCands)),\n",
    "            \"lpt\": pad(np.log(jets.PFCands.pt)),\n",
    "            \"lptf\": pad(np.log(jets.PFCands.pt / jets.pt)),\n",
    "            \"f1\": pad(np.log(np.abs(jets.PFCands.d0) + 1)),\n",
    "            \"f2\": pad(np.log(np.abs(jets.PFCands.dz) + 1)),\n",
    "        },\n",
    "        \"mask\": {\n",
    "            \"mask\": pad(ak.ones_like(jets.PFCands.pt)),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Compacting the array elements into the desired dimension using\n",
    "    # ak.concatenate\n",
    "    retmap = {\n",
    "        k: ak.concatenate([x[:, np.newaxis, :] for x in imap[k].values()], axis=1)\n",
    "        for k in imap.keys()\n",
    "    }\n",
    "\n",
    "    # Returning everything using a dictionary. Also take care of type\n",
    "    # conversion here.\n",
    "    return (), {\n",
    "        \"points\": ak.values_astype(retmap[\"points\"], \"float32\"),\n",
    "        \"features\": ak.values_astype(retmap[\"features\"], \"float32\"),\n",
    "        \"mask\": ak.values_astype(retmap[\"mask\"], \"float16\"),\n",
    "    }\n",
    "\n",
    "\n",
    "class ParticleNetExample(torch_wrapper):\n",
    "    def prepare_awkward_to_numpy(self, events):\n",
    "        jets = ak.flatten(events.Jet)\n",
    "        return jet_features_as_numpy(jets)\n",
    "\n",
    "    def numpy_to_awkward(self, return_array, events):\n",
    "        softmax = np.exp(return_array)[:, 0] / np.sum(np.exp(return_array), axis=-1)\n",
    "\n",
    "        njets = ak.count(ak.typetracer.length_one_if_typetracer(events.Jet.pt), axis=-1)\n",
    "        if ak.backend(events) == \"typetracer\":\n",
    "            njets = ak.full_like(njets, 1)\n",
    "        out = ak.unflatten(softmax, njets)\n",
    "        if ak.backend(events) == \"typetracer\":\n",
    "            out = ak.Array(\n",
    "                out.layout.to_typetracer(forget_length=True), behavior=out.behavior\n",
    "            )\n",
    "        return ak.unflatten(softmax, njets)\n",
    "\n",
    "\n",
    "pn_example = ParticleNetExample(\"model.pt\")\n",
    "\n",
    "# Running on awkward arrays\n",
    "ak_events = open_events(permit_dask=False)\n",
    "ak_jets = ak_events.Jet\n",
    "ak_jets[\"MLresults\"] = pn_example(dask_events)\n",
    "ak_events[\"Jet\"] = ak_jets\n",
    "\n",
    "# Running on dask awkward arrays\n",
    "dask_events = open_events(permit_dask=True)\n",
    "dask_jets = dask_events.Jet\n",
    "dask_jets[\"MLresults\"] = pn_example(dask_events)\n",
    "dask_events[\"Jet\"] = dask_jets\n",
    "\n",
    "# Checking that we get identical results\n",
    "print(dask_events.Jet.MLresults.compute())\n",
    "assert ak.all(dak_results.Jet.MLresults.compute() == ak_events.Jet.MLresults)\n",
    "\n",
    "# Check which columns are loaded\n",
    "print(dak.necessary_columns(dask_events.Jet.MLresults))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you feel uncertain about the implementation of the `numpy_to_awkward` method.\n",
    "You can also write the class so that the folding occurs outside the wrapper class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleNetExample_Alt(torch_wrapper):\n",
    "    def prepare_awkward_to_numpy(self, jets):\n",
    "        return jet_features_as_numpy(jets)\n",
    "\n",
    "    def numpy_to_awkward(self, return_array, jets):\n",
    "        softmax = np.exp(return_array)[:, 0] / np.sum(np.exp(return_array), axis=-1)\n",
    "        return ak.from_numpy(softmax)\n",
    "\n",
    "\n",
    "pn_example_alt = ParticleNetExample_Alt(\"model.pt\")\n",
    "\n",
    "# Running on awkward arrays\n",
    "ak_events = open_events(permit_dask=False)\n",
    "ak_njets = ak.num(ak_events.Jet, axis=-1)\n",
    "ak_jets = ak.flatten(ak_events.Jet)\n",
    "ak_jets[\"MLresults\"] = pn_example(ak_jets)\n",
    "ak_events[\"Jet\"] = ak.unflatten(ak_jets, ak_njets)\n",
    "\n",
    "# Running on dask awkward arrays\n",
    "dask_events = open_events(permit_dask=True)\n",
    "dask_njets = dak.num(dask_events.Jet, axis=-1)\n",
    "dask_jets = dak.flatten(dask_events.Jet)\n",
    "dask_jets[\"MLresults\"] = pn_example(dask_jets)\n",
    "dask_events[\"Jet\"] = ak.unflatten(dask_jets, dask_njets)\n",
    "\n",
    "# Checking that we get identical results\n",
    "print(dask_events.Jet.MLresults.compute())\n",
    "assert ak.all(dask_events.Jet.MLresults.compute() == ak_events.Jet.MLresults)\n",
    "\n",
    "# Check which columns are loaded\n",
    "print(dak.necessary_columns(dask_events.Jet.MLresults))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments about generalizing to other ML tools\n",
    "\n",
    "All ML wrappers provided in the `coffea.mltools` module (`triton_wrapper` for\n",
    "[triton][triton] server inference, `torch_wrapper` for pytorch, and\n",
    "`xgboost_wrapper` for [xgboost][xgboost] inference) follow the same design:\n",
    "analyzers is responsible for providing the model of interest, along with\n",
    "providing an inherited class that overloads of the following methods to data\n",
    "type conversion:\n",
    "\n",
    "- `awkward_to_numpy`: converting awkward arrays to `numpy` arrays, the output\n",
    "  `numpy` arrays should be in the format of a tuple `a` and a dictionary `b`,\n",
    "  which can be expanded out to the input of the ML tool like `model(*a, **b)`.\n",
    "  Notice some additional trivial conversion (like converting to available\n",
    "  kernels for `pytorch`, converting to a matrix format for `xgboost`, and slice\n",
    "  of array for `triton` is handled automatically by the respective wrappers)\n",
    "- `numpy_to_awkward` (optional): converting the number results back to awkward\n",
    "  array format. If this is not provided, then a simple `ak.from_numpy`\n",
    "  conversion takes place.\n",
    "- `dask_columns` (optional but recommended): Given the inputs to the\n",
    "  `awkward_to_numpy` method, list the branches required for the inference\n",
    "  calculation. If not provided, it will attempt to load all branches\n",
    "  recursively, which may have significant performance penalties.\n",
    "\n",
    "If the ML tool of choice for your analysis has not been implemented by the\n",
    "`coffea.mltools` modules, consider constructing your own with the provided\n",
    "`numpy_call_wrapper` base class in `coffea.mltools`. Aside from the functions\n",
    "listed above, you will also need to provide the `numpy_call` method to perform\n",
    "any additional data format conversions, and call the ML tool of choice. If you\n",
    "think your implementation is general, also consider submitting a PR to the\n",
    "`coffea` repository!\n",
    "\n",
    "[triton]: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver\n",
    "[xgboost]: https://xgboost.readthedocs.io/en/stable/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffea-developement",
   "language": "python",
   "name": "coffea-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
