{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running inference tools\n",
    "\n",
    "As machine learning (ML) becomes more popular in HEP analysis, coffea also\n",
    "provide tools to assist with using ML tools within the coffea framework. For\n",
    "training and validation, you would likely need custom data mangling tools to\n",
    "convert standard CMS data formats ([NanoAOD][nanoaod], [PFNano][pfnano]) to a\n",
    "format that best interfaces with the ML tool of choice to have fine control over\n",
    "what is done. For more advanced use cases of data mangling and data saving,\n",
    "refer to the [awkward array manual][datamangle] and\n",
    "[uproot][uproot_write]/[parquet][ak_parquet] write operations for saving\n",
    "intermediate states. This reference mainly focuses on the result inference side\n",
    "of ML tools, where ML tool outputs are used as another variable to be used in\n",
    "the event/object selection chain.\n",
    "\n",
    "[nanoaod]: https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookNanoAOD\n",
    "[pfnano]: https://github.com/cms-jet/PFNano\n",
    "[datamangle]: https://awkward-array.org/doc/main/user-guide/how-to-restructure.html\n",
    "[uproot_write]: https://uproot.readthedocs.io/en/latest/basic.html#writing-ttrees-to-a-file\n",
    "[ak_parquet]: https://awkward-array.org/doc/main/reference/generated/ak.to_parquet.html\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why these wrapper tools are needed\n",
    "\n",
    "The typical operation of using ML inference tools in the awkward/coffea analysis\n",
    "tools involves the conversion and padding of awkward array to ML tool containers\n",
    "(usually something that is `numpy`-compatible), run the inference, then\n",
    "convert-and-truncate back into the awkward array syntax required for the\n",
    "analysis chain to continue. With awkward arrays' laziness now being handled\n",
    "entirely by [`dask`][dask_awkward], the conversion operation of awkward array to\n",
    "other array types needs to be wrapped in a way that is understandable to `dask`.\n",
    "The packages in the `ml_tools` package attempts to wrap the common tools used by\n",
    "the HEP community with a common interface to reduce the verbosity of the code on\n",
    "the analysis side.\n",
    "\n",
    "[dask_awkward]: https://dask-awkward.readthedocs.io/en/stable/gs-limitations.html\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using ParticleNet-like jet variable calculation using PyTorch\n",
    "\n",
    "The example given in this notebook be using [`pytorch`][pytorch] to calculate a\n",
    "jet-level discriminant using its constituent particles. An example for how to\n",
    "construct such a `pytorch` network can be found in the docs file, but for\n",
    "`mltools` in coffea, we only support the [TorchScript][pytorch] format files to\n",
    "load models to ensure operability when scaling to clusters. Let us first start\n",
    "by downloading the example ParticleNet model file, as well as construct a dummy\n",
    "Event-Jet-PFCandidate nested jagged arrays (Notice that this is a random array\n",
    "with actual physical meaning, and is just used as an example for array\n",
    "structure).\n",
    "\n",
    "\n",
    "[pytorch]: https://pytorch.org/\n",
    "[pytorch_jit]: https://pytorch.org/tutorials/beginner/saving_loading_models.html#export-load-model-in-torchscript-format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --quiet -O model.pt https://github.com/CoffeaTeam/coffea/raw/ml_tools/tests/samples/triton_models_test/pn_test/1/model.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import awkward as ak\n",
    "\n",
    "NEVT = 10\n",
    "NJETS = (NEVT, 4)\n",
    "NCANDS = (*NJETS, 100)\n",
    "\n",
    "\n",
    "def make_events_array():\n",
    "    def make_ak(arr):\n",
    "        return ak.from_regular(arr, axis=(arr.ndim - 1))\n",
    "\n",
    "    # Creating randomized nested-structure\n",
    "    events = ak.zip(\n",
    "        {\n",
    "            \"HT\": make_ak(np.random.exponential(scale=1000, size=NEVT)),\n",
    "            \"NJets\": make_ak(np.random.randint(1, NJETS[-1], size=NEVT)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    Jets = ak.zip(\n",
    "        {\n",
    "            \"pt\": make_ak(np.random.exponential(20, size=NJETS)),\n",
    "            \"eta\": make_ak(np.random.normal(2, size=NJETS)),\n",
    "            \"phi\": make_ak(np.random.uniform(-np.pi, np.pi, size=NJETS)),\n",
    "            \"NPFCands\": make_ak(np.random.randint(1, NCANDS[-1], size=NJETS)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    PFCands = ak.zip(\n",
    "        {\n",
    "            \"pt\": make_ak(np.random.exponential(1, size=NCANDS)),\n",
    "            \"eta\": make_ak(np.random.normal(2, size=NCANDS)),\n",
    "            \"phi\": make_ak(np.random.uniform(-np.pi, np.pi, size=NCANDS)),\n",
    "            \"feat1\": make_ak(np.random.random(size=NCANDS)),\n",
    "            \"feat2\": make_ak(np.random.random(size=NCANDS)),\n",
    "            \"feat3\": make_ak(np.random.random(size=NCANDS)),\n",
    "            \"feat4\": make_ak(np.random.random(size=NCANDS)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Making nested jagged structure\n",
    "    Jets[\"PFCands\"] = PFCands[ak.local_index(PFCands.pt, axis=-1) < Jets.NPFCands]\n",
    "    events[\"Jets\"] = Jets[ak.local_index(Jets.pt, axis=-1) < events.NJets]\n",
    "    return events\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare a class to handle inference request. Here we import the base\n",
    "`torch_wrapper` class and create a new class that inherits `torch_wrapper`. As\n",
    "the class cannot know anything about the data mangling required, we will need to\n",
    "overload at least the method `awkward_to_numpy`:\n",
    "\n",
    "- The input can be an arbitrary number of awkward arrays. Here we will be\n",
    "  passing in the event array in the format constructed as the format array.\n",
    "- The output should be single tuple `a` and single dictionary `b`, this is to\n",
    "  ensure that arbitrarily complicated outputs can be passed to the underlying\n",
    "  `pytorch` model instance like `model(*a, **b)`. The contents of `a` and `b`\n",
    "  will need to be determined by the model of interest. In our ParticleNet-like\n",
    "  example, the model expects the following inputs:\n",
    "\n",
    "  - A `N` jets x `2` coordinate x `100` constituents \"points\" array,\n",
    "    representing the constituent coordinates.\n",
    "  - A `N` jets x `5` feature x `100` constituents \"features\" array, representing\n",
    "    the constituent features of interest to be used for inference.\n",
    "  - A `N` jets x `1` mask x `100` constituent \"mask\" array, representing whether\n",
    "    a constituent should be masked from the inference request.\n",
    "\n",
    "  The user is also responsible to making sure the data format is compatible with\n",
    "  the model of interest. Defining this minimum class, we can attempt to run an\n",
    "  inference using the `__call__` method of our defined class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ensc/VirtualENV/coffea-test/lib/python3.8/site-packages/coffea/ml_tools/helper.py:155: UserWarning: No format checks were performed on input!\n",
      "  warnings.warn(\"No format checks were performed on input!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0511, -0.0321], [0.0543, -0.00448], ..., [0.0513, ...], [0.0511, -0.0354]]\n",
      "<class 'awkward.highlevel.Array'>\n",
      "<bound method Array.__repr__ of <Array [[0.0511, -0.0321], ..., [0.0511, -0.0354]] type='15 * 2 * float32'>>\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "from coffea.ml_tools.torch_wrapper import torch_wrapper\n",
    "\n",
    "\n",
    "class ParticleNetExample1(torch_wrapper):\n",
    "    def awkward_to_numpy(self, events):\n",
    "        jets = ak.flatten(events.Jets)\n",
    "\n",
    "        def pad(arr):\n",
    "            return ak.fill_none(ak.pad_none(arr, 100, axis=1, clip=True), 0.0,)\n",
    "\n",
    "        # Human readable version of what the inputs are\n",
    "        # Each array is a N jets x 100 constituent arry\n",
    "        imap = {\n",
    "            \"points\": {\n",
    "                \"deta\": pad(jets.eta - jets.PFCands.eta),\n",
    "                \"dphi\": pad(jets.phi - jets.PFCands.phi),\n",
    "            },\n",
    "            \"features\": {\n",
    "                \"dr\": pad(\n",
    "                    np.sqrt(\n",
    "                        (jets.eta - jets.PFCands.eta) ** 2\n",
    "                        + (jets.phi - jets.PFCands.phi) ** 2\n",
    "                    )\n",
    "                ),\n",
    "                \"lpt\": pad(np.log(jets.PFCands.pt)),\n",
    "                \"lptf\": pad(\n",
    "                    np.log(jets.PFCands.pt / ak.sum(jets.PFCands.pt, axis=-1))\n",
    "                ),\n",
    "                \"f1\": pad(np.log(jets.PFCands.feat1 + 1)),\n",
    "                \"f2\": pad(np.log(jets.PFCands.feat2 + 1)),\n",
    "            },\n",
    "            \"mask\": {\n",
    "                \"mask\": pad(ak.ones_like(jets.PFCands.pt)),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Compacting the array elements into the desired dimension using\n",
    "        # ak.concatenate\n",
    "        retmap = {\n",
    "            k: ak.concatenate(\n",
    "                [x[:, np.newaxis, :] for x in imap[k].values()], axis=1\n",
    "            ).to_numpy()\n",
    "            for k in imap.keys()\n",
    "        }\n",
    "        \n",
    "        # Returning everything using a dictionary. Also take care of type\n",
    "        # conversion here.\n",
    "        return (), {\n",
    "            \"points\": retmap[\"points\"].astype(np.float32),\n",
    "            \"features\": retmap[\"features\"].astype(np.float32),\n",
    "            \"mask\": retmap[\"mask\"].astype(np.float16),\n",
    "        }\n",
    "\n",
    "\n",
    "pn_example1 = ParticleNetExample1(\"model.pt\")\n",
    "events = make_events_array()\n",
    "results = pn_example1(events)\n",
    "print(results)\n",
    "print(type(results))\n",
    "print(results.__repr__)\n",
    "print(ak.count(events.Jets.pt))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each jet in the input to the `torch` model, the model returns 2-score\n",
    "elements. Without additional specification, the `torch_wrapper` class performs a\n",
    "trival conversion of `ak.from_numpy` using the torch outputs. We can specify\n",
    "that we want to fold this back into nested structure by overloading the\n",
    "`numpy_to_awkward` method of the class. \n",
    "\n",
    "For this example we are going perform additional computation for the conversion\n",
    "back to awkward array formats: \n",
    "\n",
    "- Calculate the `softmax` method for the return 0 of each jets (commonly used\n",
    "  for ML output ``scores'')\n",
    "- Fold the computed softmax array back into nested structure that is compatible\n",
    "  with the original inputs events array.\n",
    "\n",
    "Notice that the inputs of the `numpy_to_awkward` method is different from the\n",
    "`awkward_to_numpy` method, only by that the first argument is the return numpy\n",
    "array of the model inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.521], [0.515, 0.522, 0.522], [0.52], ..., [0.521], [0.521], [0.52, 0.522]]\n"
     ]
    }
   ],
   "source": [
    "class ParticleNetExample2(ParticleNetExample1):\n",
    "    def numpy_to_awkward(self, return_array, events):\n",
    "        softmax = np.exp(return_array)[:, 0] / np.sum(np.exp(return_array), axis=-1)\n",
    "\n",
    "        njets = ak.count(events.Jets.pt, axis=-1)\n",
    "        return ak.unflatten(softmax, njets)\n",
    "\n",
    "\n",
    "pn_example2 = ParticleNetExample2(\"model.pt\")\n",
    "jets = events.Jets\n",
    "jets[\"MLresults\"] = pn_example2(events)\n",
    "events[\"Jets\"] = jets\n",
    "\n",
    "print(events.Jets.MLresults)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a per-jet variable we can use to continue the event/object selection\n",
    "chain for our analysis! \n",
    "\n",
    "Notice that up till now, we have been working exclusively with plain awkward.\n",
    "But this class is already ready to be extended to dask awkward! The `__call__`\n",
    "method commonly knows how to handle the different array types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ensc/VirtualENV/coffea-test/lib/python3.8/site-packages/coffea/ml_tools/helper.py:306: UserWarning: Touching all columns recursively may have performance penaltiesConsider overloading the dask_columns method of ParticleNetExample2\n",
      "  warnings.warn(\n",
      "/home/ensc/VirtualENV/coffea-test/lib/python3.8/site-packages/coffea/ml_tools/helper.py:155: UserWarning: No format checks were performed on input!\n",
      "  warnings.warn(\"No format checks were performed on input!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.521], [0.515, 0.522, 0.522], [0.52], ..., [0.521], [0.521], [0.52, 0.522]]\n",
      "True\n",
      "{'read-parquet-919fe6c2d5b954ae9b92e0d2895dc5f0': ['Jets.PFCands.eta', 'Jets.pt', 'Jets.PFCands.feat4', 'NJets', 'Jets.eta', 'Jets.PFCands.phi', 'Jets.PFCands.feat3', 'Jets.PFCands.pt', 'Jets.PFCands.feat2', 'Jets.phi', 'Jets.MLresults', 'Jets.NPFCands', 'HT', 'Jets.PFCands.feat1']}\n"
     ]
    }
   ],
   "source": [
    "import dask_awkward as dak\n",
    "\n",
    "# Creating a lazy dask array of events array with the same\n",
    "ak.to_parquet(events, \"events.parquet\")\n",
    "dask_events = dak.from_parquet(\"events.parquet\")\n",
    "\n",
    "# Syntax for dask arrays is identical to the plain awkward arrays!\n",
    "dask_jets = dask_events.Jets\n",
    "dask_jets[\"MLresults_dask\"] = pn_example2(dask_events)\n",
    "dask_events[\"Jets\"] = dask_jets\n",
    "\n",
    "# Checking that we get identical results\n",
    "print(dask_events.Jets.MLresults_dask.compute())\n",
    "print(\n",
    "    ak.all(\n",
    "        dask_events.Jets.MLresults_dask.compute()\n",
    "        == dask_events.Jets.MLresults.compute()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Check which columns are loaded\n",
    "print(dak.necessary_columns(dask_events.Jets.MLresults_dask))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only remaining issue is that `dask` is currently loading all columns in the\n",
    "array that is passed to the `awkward_to_numpy` method, even those that are not\n",
    "required for the ML inference computation. While this behavior ensure that\n",
    "everything is loaded, leading to less room for errors to during the development\n",
    "of the `awkward_to_numpy` method, leaving this behavior unchanged can load to\n",
    "excessive memory usage, depending on how the event array is set up.\n",
    "\n",
    "To properly solve this behavior, one should further overload the `dask_columns`\n",
    "method to return a list of columns that is strictly needed by the inference\n",
    "tools. The inputs should be identical to that of the `awkward_to_numpy` method\n",
    "defined by the user. \n",
    "\n",
    "*Note:* as of writing, there is a [bug][bug] that causes the kernel to\n",
    "segmentation fault if the required branches are not included. If you are running\n",
    "into issues, it might be that you need to allow the class you are using to be\n",
    "slightly less lazy.\n",
    "\n",
    "[bug]: https://github.com/dask-contrib/dask-awkward/issues/249\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.522, 0.518, 0.52], [0.52], ..., [0.521, ..., 0.521], [0.518, 0.521, 0.52]]\n",
      "{'read-parquet-d2173ecf0056d81c95a83d0ff5c0fe1a': ['Jets.PFCands.eta', 'Jets.pt', 'Jets.PFCands.feat4', 'Jets.eta', 'Jets.PFCands.phi', 'Jets.PFCands.feat3', 'Jets.PFCands.pt', 'Jets.PFCands.feat2', 'Jets.phi', 'Jets.NPFCands', 'Jets.PFCands.feat1']}\n"
     ]
    }
   ],
   "source": [
    "class ParticleNetExample3(ParticleNetExample2):\n",
    "    def dask_columns(self, events):\n",
    "        return [\n",
    "            events.Jets.pt,\n",
    "            events.Jets.eta,\n",
    "            events.Jets.phi,\n",
    "            events.Jets.NPFCands,\n",
    "            events.Jets.PFCands,\n",
    "        ]\n",
    "\n",
    "\n",
    "pn_example3 = ParticleNetExample3(\"model.pt\")\n",
    "\n",
    "# Reloading a lazy instance of the array\n",
    "ak.to_parquet(make_events_array(), \"events2.parquet\")\n",
    "dask_events = dak.from_parquet(\"events2.parquet\")\n",
    "\n",
    "# Syntax for dask arrays is identical to the plain awkward arrays!\n",
    "dask_jets = dask_events.Jets\n",
    "dask_jets[\"MLresults_lazy\"] = pn_example3(dask_events)\n",
    "dask_events[\"Jets\"] = dask_jets\n",
    "\n",
    "# Checking that we get identical results\n",
    "print(dask_events.Jets.MLresults_lazy.compute())\n",
    "\n",
    "# Check which columns are loaded\n",
    "print(dak.necessary_columns(dask_events.Jets.MLresults_lazy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the implementation of the classes above can be written in a single\n",
    "class. Here is a copy-and-paste implementation of the class with all the\n",
    "functionality described in the cells above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.519], [0.52, 0.519, 0.521], [...], ..., [0.518], [0.521, 0.521, 0.519]]\n",
      "{'read-parquet-919fe6c2d5b954ae9b92e0d2895dc5f0': ['Jets.PFCands.eta', 'Jets.pt', 'Jets.PFCands.feat4', 'Jets.eta', 'Jets.PFCands.phi', 'Jets.PFCands.feat3', 'Jets.PFCands.pt', 'Jets.PFCands.feat2', 'Jets.phi', 'Jets.NPFCands', 'Jets.PFCands.feat1']}\n"
     ]
    }
   ],
   "source": [
    "class ParticleNetExample(torch_wrapper):\n",
    "    def awkward_to_numpy(self, events):\n",
    "        jets = ak.flatten(events.Jets)\n",
    "\n",
    "        def pad(arr):\n",
    "            return ak.fill_none(\n",
    "                ak.pad_none(arr, 100, axis=1, clip=True),\n",
    "                0.0,\n",
    "            )\n",
    "\n",
    "        # Human readable version of what the inputs are\n",
    "        # Each array is a N jets x 100 constituent arry\n",
    "        imap = {\n",
    "            \"points\": {\n",
    "                \"deta\": pad(jets.eta - jets.PFCands.eta),\n",
    "                \"dphi\": pad(jets.phi - jets.PFCands.phi),\n",
    "            },\n",
    "            \"features\": {\n",
    "                \"dr\": pad(\n",
    "                    np.sqrt(\n",
    "                        (jets.eta - jets.PFCands.eta) ** 2\n",
    "                        + (jets.phi - jets.PFCands.phi) ** 2\n",
    "                    )\n",
    "                ),\n",
    "                \"lpt\": pad(np.log(jets.PFCands.pt)),\n",
    "                \"lptf\": pad(np.log(jets.PFCands.pt / ak.sum(jets.PFCands.pt, axis=-1))),\n",
    "                \"f1\": pad(np.log(jets.PFCands.feat1 + 1)),\n",
    "                \"f2\": pad(np.log(jets.PFCands.feat2 + 1)),\n",
    "            },\n",
    "            \"mask\": {\n",
    "                \"mask\": pad(ak.ones_like(jets.PFCands.pt)),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Compacting the array elements into the desired dimension using\n",
    "        # ak.concatenate\n",
    "        retmap = {\n",
    "            k: ak.concatenate(\n",
    "                [x[:, np.newaxis, :] for x in imap[k].values()], axis=1\n",
    "            ).to_numpy()\n",
    "            for k in imap.keys()\n",
    "        }\n",
    "\n",
    "        # Returning everything using a dictionary. Also take care of type\n",
    "        # conversion here.\n",
    "        return (), {\n",
    "            \"points\": retmap[\"points\"].astype(np.float32),\n",
    "            \"features\": retmap[\"features\"].astype(np.float32),\n",
    "            \"mask\": retmap[\"mask\"].astype(np.float16),\n",
    "        }\n",
    "\n",
    "    def numpy_to_awkward(self, return_array, events):\n",
    "        softmax = np.exp(return_array)[:, 0] / np.sum(np.exp(return_array), axis=-1)\n",
    "\n",
    "        njets = ak.count(events.Jets.pt, axis=-1)\n",
    "        return ak.unflatten(softmax, njets)\n",
    "\n",
    "    def dask_columns(self, events):\n",
    "        return [\n",
    "            events.Jets.pt,\n",
    "            events.Jets.eta,\n",
    "            events.Jets.phi,\n",
    "            events.Jets.NPFCands,\n",
    "            events.Jets.PFCands,\n",
    "        ]\n",
    "\n",
    "\n",
    "pn_example = ParticleNetExample(\"model.pt\")\n",
    "\n",
    "# Reloading a lazy instance of the array\n",
    "ak.to_parquet(make_events_array(), \"events.parquet\")\n",
    "dask_events = dak.from_parquet(\"events.parquet\")\n",
    "\n",
    "# Syntax for dask arrays is identical to the plain awkward arrays!\n",
    "dask_jets = dask_events.Jets\n",
    "dask_jets[\"MLresults\"] = pn_example(dask_events)\n",
    "dask_events[\"Jets\"] = dask_jets\n",
    "\n",
    "# Checking that we get identical results\n",
    "print(dask_events.Jets.MLresults.compute())\n",
    "\n",
    "# Check which columns are loaded\n",
    "print(dak.necessary_columns(dask_events.Jets.MLresults))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments about generalizing to other ML tools\n",
    "\n",
    "All ML wrappers provided in the `coffea.mltools` module (`triton_wrapper` for\n",
    "[triton][triton] server inference, `torch_wrapper` for pytorch, and\n",
    "`xgboost_wrapper` for [xgboost][xgboost] inference) follow the same design:\n",
    "analyzers is responsible for providing the model of interest, along with\n",
    "providing an inherited class that overloads of the following methods to data\n",
    "type conversion:\n",
    "\n",
    "- `awkward_to_numpy`: converting awkward arrays to `numpy` arrays, the output\n",
    "  `numpy` arrays should be in the format of a tuple `a` and a dictionary `b`,\n",
    "  which can be expanded out to the input of the ML tool like `model(*a, **b)`.\n",
    "  Notice some additional trivial conversion (like converting to available\n",
    "  kernels for `pytorch`, converting to a matrix format for `xgboost`, and slice\n",
    "  of array for `triton` is handled automatically by the respective wrappers)\n",
    "- `numpy_to_awkward` (optional): converting the number results back to awkward\n",
    "  array format. If this is not provided, then a simple `ak.from_numpy`\n",
    "  conversion takes place.\n",
    "- `dask_columns` (optional but recommended): Given the inputs to the\n",
    "  `awkward_to_numpy` method, list the branches required for the inference\n",
    "  calculation. If not provided, it will attempt to load all branches\n",
    "  recursively, which may have significant performance penalties.\n",
    "\n",
    "If the ML tool of choice for your analysis has not been implemented by the\n",
    "`coffea.mltools` modules, consider constructing your own with the provided\n",
    "`numpy_call_wrapper` base class in `coffea.mltools`. Aside from the functions\n",
    "listed above, you will also need to provide the `numpy_call` method to perform\n",
    "any additional data format conversions, and call the ML tool of choice. If you\n",
    "think your implementation is general, also consider submitting a PR to the\n",
    "`coffea` repository!\n",
    "\n",
    "[triton]: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver\n",
    "[xgboost]: https://xgboost.readthedocs.io/en/stable/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
